---
title: "Heter Mimic"
output: html_document
date: "2025-07-15"
---

```{r setup, include=FALSE}
#Libraries set-up
library(benchtm)
library(dplyr)
set.seed(12345)
library(tidyverse)
library(nnls)
library(SuperLearner)
library(permimp)
library(dplyr)
library(clustermq)
library(ggplot2)
library(ggpubr)
library(GGally)
library(vivid)
library(tableone)
library(quadprog)
library(naniar)
library(caret)
library(xgboost)
```

## Step 1

#S1 where beta =1

```{r data S1}
#Generate 30 Xs from data_syn
scal <- 1
X <- generate_X_syn(n=500* scal)
trt <- generate_trt(n=500* scal, p_trt = 0.5)

```

```{r}
##Model setup
##Add Region(X31)
probs <- c(N = 0.75, O = 0.25)  
region <- rmultinom(n = 1, size = 500, prob = probs)
X$Xregion <- factor(rep(names(probs), times = region[,1])) 

X$X31 <- NA  # Initialize

X$X31[X$Xregion == "N"] <- 0
X$X31[X$Xregion == "O"] <- 1

table(X$X31)

```

```{r}
# Assign U(X32) based on region
X$X32[X$Xregion == "N"] <- sample(c(0, 1), size = sum(X$Xregion == "N"), replace = TRUE, prob = c(0.90, 0.10))
X$X32[X$Xregion == "O"] <- sample(c(0, 1), size = sum(X$Xregion == "O"), replace = TRUE, prob = c(0.20, 0.80))

cor(X$X32,X$X31)

table(X$X32,X$X31)
prop.table(table(X$X32, X$X31), margin = 2)  # Check proportions by region

ggplot(X, aes(x = factor(X31), fill = factor(X32))) +
  geom_bar(position = "fill") +
  labs(
    title = "Proportion of U within Each Level of X31",
    x = "X31",
    y = "Proportion",
    fill = "X32"
  ) +
  theme_minimal()
```

```{r }
#Define prog and pred function and calculate b1
prog <- "0.5*X32"
pred <- "X32"
get_b(X, scal,prog, pred, trt,
      type = "continuous",
      power = c(0.5, 0.8), alpha = c(0.025, 0.1),
      start = c(0, 0), sigma_error = 1
)

summary(X$X32)

#We have b1*=0.2049954.Then we consider five situations: b1=0,b1=0.5*(b1*),b1=1*(b1*),b1=1.5*(b1*),b1=2*(b1*).
```

```{r cleaning, echo=FALSE}
#We simulate b1=2*(b1*).
get_b0(X, scal, prog, pred, trt,
       b1 = 0.49894661*2, type = "continuous",
       power = 0.5, alpha = 0.025, interval = c(-2, 2), sigma_error = 1
)
#We have b0=0.3325223.
```

```{r cleaning, echo=FALSE}
#Calculate response variable based on b0 and b1
dat <- generate_y(X, trt, prog ,
                  pred , b0 = -0.0794497, b1 =0.49894661*2,
                  type = "continuous", sigma_error = 3)

#Recode categorical variables as 0/1
dat$X1 <- ifelse(dat$X1 == "Y", 1, 0)
dat$X2 <- ifelse(dat$X2 == "Y", 1, 0)
dat$X4 <- ifelse(dat$X4 == "Y", 1, 0)
dat$X6 <- ifelse(dat$X6 == "Y", 1, 0)
dat$X7 <- ifelse(dat$X7 == "Y", 1, 0)
dat$X8 <- ifelse(dat$X8 == "Y", 1, 0)
dat$X9 <- ifelse(dat$X9 == "F", 1, 0)
levels <- c("a", "b", "c", "d", "e")
dat$X3 <- match(dat$X3, levels) - 1

# Remove Xregion
dat$Xregion <- NULL

```

```{r generate data and missing}
#### data part: data generated on different scenarios saved in "scen_param" from library(benchtm)
#data(scen_param)

#cases <- scen_param %>% filter(type == "continuous" &
                                 #pred == "(X14 > 0.25) & (X1 == 'N')" &
                                 #b1_rel == 2)

## generate data using benchtm
#dat <- generate_scen_data(scen = cases,  include_truth = F) %>%
  #mutate_if(is.character, as.factor) %>%
  #mutate_at('trt', as.factor)

# set.seed(10)
# cases <- (scen_param %>% filter(type == "continuous"))
# cases[15, ]$pred
# dat <- generate_scen_data(scen = cases[15, ]) %>% 
#   mutate_if(is.character, as.factor) %>%
#   select(-c(trt_effect, prob_diff)) %>%
#   mutate_at('trt', as.factor)

# Remove Xregion
dat$Xregion <- NULL
dat$region_effect <- NULL

names(dat)[names(dat) == "X31"] <- "Region"


X <- dat %>% dplyr::select(starts_with("X"))
Y <- dat$Region
trt <- dat$trt

## randomly generate missing value for baseline covariate with missing proportion 5%, 10%, 20% for X2, X5, X6 
## (for demonstration of data preprocessing)
X2_miss_index <- rbinom(n = nrow(X), size = 1, p = 0.05)
X5_miss_index <- rbinom(n = nrow(X), size = 1, p = 0.10)
X6_miss_index <- rbinom(n = nrow(X), size = 1, p = 0.20)

X[which(X2_miss_index == 1), 2] <- NA
X[which(X5_miss_index == 1), 5] <- NA
X[which(X6_miss_index == 1), 6] <- NA

dat <- cbind(Y, trt, X) %>% mutate_at("trt", as.factor)

saveRDS(dat, file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/analysis_data.rds")
```

## Step 2

```{r ida, echo=FALSE}
adat <- dat
readRDS(file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/analysis_data.rds")

#######################################################################
## Reproduce existing analyses
#######################################################################
## First step is to cross-check the number of patients in the analysis
## and across treatment groups and reproduce already published analyses
## on the same data-set (e.g. the primary analysis for a specific trial)
## to make sure that the right data are being used in the right way.

## code here

#######################################################################
## Univariate summary of baseline variables
#######################################################################
## Investigate basic properties of the distribution of baseline variables
## (mean, variability/information, skewness, outliers, missingness).
## Results may suggest to use transformations (e.g. log transform) for
## some variables.

# ## first text based summaries
# tab <- tableone::CreateTableOne(data = adat)
# summary(tab)

## histogram/barplots of all variables (save plots as pdf under reports/ for easier review)
## for paper, only draw plot with 2 variables
vars <- c("X5", "X6")
pp_plots <- lapply(vars, plot_density_bar, data = adat)
res <- ggpubr::ggarrange(pp_plots[[1]], pp_plots[[2]], nrow = 1, ncol = 2)

# Check is the reports folder exists, and if not create it
#dir.create(file.path("reports"))
# Check is the reports/figures folder exists, and if not create it
dir.create(file.path("/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures"))
ggsave("/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/ida_a.pdf",
       res,
       width = 6, height = 4, units = "in"
)

#######################################################################
## Stratified univariate summary of baseline variables
#######################################################################
## Observe baseline summaries stratified by another categorical factor.
## Typical stratification factors are "study or "treatment group".
## In this case we want to compare placebo vs Cosentyx 300mg
## but Cosentyx 300mg is not available in all studies. So both
## treatment and study are of interest as stratification factors

## text based summary
tab <- tableone::CreateTableOne(strata = "trt", data = adat)
print(tab)
# tab <- tableone::CreateTableOne(strata = "STUDYID", data = adat)
# print(tab)

## side-by-side boxplot or stacked bar-plots
vars <- c("X5", "X6")
pp_plots <- lapply(vars, plot_boxbar_by, y.var = "trt", data = adat)

res <- ggpubr::ggarrange(pp_plots[[1]], pp_plots[[2]], nrow = 1, ncol = 2)

ggsave("/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/ida_b.pdf",
       res,
       width = 6, height = 4, units = "in"
)

#######################################################################
## Evalulate missing values/non-informative baseline variables
#######################################################################
## Here missingness & missingness patterns are explored further, in
## addition to assessment of variables with low information (e.g. all
## observations of one variable equal to one value). Both may suggest
## removal of certain variables. In addition for categorical variables
## it is observed whether there are variables with sparsely populated
## categories (may merge those categories with other categories).

## missing variables
p1 <- naniar::gg_miss_var(adat %>% select(X1, X2, X3, X4, X5, X6, X7, X8), 
                          show_pct = TRUE)
## missing variable patterns
p2 <- naniar::vis_miss(adat %>% select(X1, X2, X3, X4, X5, X6, X7, X8)) +
  coord_flip()

res <- ggpubr::ggarrange(p1, p2, nrow = 1, ncol = 2)

ggsave("/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/ida_c.pdf",
       res,
       width = 6, height = 4, units = "in"
)



## identification of uninformative variables
nzv <- caret::nearZeroVar(adat[, -1], saveMetrics = TRUE)
head(nzv %>% arrange(desc(nzv)), n = 20)

## identification of categorical variables with sparse categories
## (code in src/util/baseline_dependency.R)
#low_freq_categories(adat)

#######################################################################
## Assess dependencies across baseline variables
#######################################################################
## Provides a better understanding of the joint distribution of baseline
## variables and helps identify duplicate (or close-to-duplicate) variables in
## the data. In addition it may help with interpretation of final results.
## To assess dependency between two variables X and Y we calculate
## sqrt(X2/(X2+N)), where X2 is the chi-squared statistic comparing a model for
## p(X|Y) versus p(X).
## If both X and Y are continuous linear models can be used (adjusting for Y linearly)
## and the result is very close to the Pearson correlation.
## If both X and Y are categorical multinomial regression models can be used
## (adjusting for Y as categorical variable). This will give results close to
## Pearson's contingency coefficient.
## For mixed data both multinomial and continuous regression can be used (the
## results assessing p(Y|X) vs p(Y) and p(X|Y) vs p(X) are usually very similar.
## Downside of this approach: For non-continuous data, the maximum achievable
## value can be <1, so assess the results by data-type.

## remove outcome and some problematic variables (e.g. just 1 value)
adat_dep <- adat[, 3:dim(adat)[2]]
dependencies <- get_dep(adat_dep)

## assess dependencies by type of variable comparison
dependencies$results %>%
  filter(Comparison == "continuous" & Correlation > 0.7) %>%
  arrange(desc(Correlation))
## for categorical use lower correlation threshold (as max achievable value can be <1)
dependencies$results %>%
  filter(Comparison == "categorical" & Correlation > 0.5) %>%
  arrange(desc(Correlation))
## mixed comparisons
dependencies$results %>%
  filter(Comparison == "mixed" & Correlation > 0.5) %>%
  arrange(desc(Correlation))

## use hierarchical clustering to assess similarity of variables
hc <- hclust(as.dist(1 - dependencies$cor_mat), method = "average")
pdf(file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/ida_d.pdf", height = 4, width = 6)
plot(hc, hang = -1, xlab = NA, sub = NA, cex = 0.67)
dev.off()

```

```{r pressure, echo=FALSE}
plot(pressure)
```

## Step 3

```{r preprocessing, echo=FALSE}

#adat <- readRDS(file = "/Users/sarah/Downloads/analysis_data.rds")

#######################################################################
## Variable removal or transformations
#######################################################################
## remove variables due to large missingness
adat <- adat %>% select(-c())
## remove variables due to uninformativeness
adat <- adat %>% select(-c())
## remove duplicated or highly correlated variables
adat <- adat %>% select(-c())
## merge sparse categories
adat <- adat 
## variable transformations
adat <- adat %>%
  mutate(
    log_X5 = log(X5 + 0.01),
    # log_X13 = log(X13),
    log_X13 = log(X13 + 0.01),
    log_X25 = log(X25 + 0.01)
  ) %>% 
  select(-c(X5, X13, X25))


## save transformed data
saveRDS(adat, file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/analysis_data2.rds")

#######################################################################
## Impute remaining missing covariates
#######################################################################
## For missing data in the *outcome* variable we propose (for consistency)
## to follow the analytical strategy used in the main pre-specified
## clinical trial analyses for this endpoint, following the decided
## intercurrent event strategies and missing data handling approaches
## (see ICH E9 addendum). In situations, where these approaches are
## complex (e.g. various multiple imputation strategies used) subsequent
## analyses may become time-consuming or infeasible. In this case we
## suggest to use simpler analyses that are still in the spirit of
## the main estimand targeted in the clinical trial analyses.
## This may for example mean using single imputation approaches.

## For missing data in *baseline* variables, we provide following
## considerations and suggestions.
## * Consider to drop covariates for missingness > 10%-20%
## * We don't recommend using multiple imputation for baseline variables
##   as it would result in multiple analysis data-sets and thus more
##   time-consuming subsequent analyses.
## * For single imputation we recommend two methods:
##   (i) Imputation of median or mode of the non-missing values from the
##       same baseline variable.
##   or better
##   (ii) Regression imputation of the missing baseline
##        covariates/biomarkers based on all other variables.
## * While there are different ways of doing (ii) above we
##   recommend to perform multiple imputation to get multiple data-sets
##   with complete baseline variables, but then, rather than using all of 
##   those data-sets in subsequent analyses, taking median/mode on the imputed 
##   values for each missing to get a single dataset. The [`mice`](https://CRAN.R-project.org/package=mice)
##   provides a lot of flexibility and ease of use for this purpose.
## * The imputation of baseline covariates should be independent of
##   outcome or any other post-baseline variables.
## * The missing value indicator method creates extra binary covariates for
##   each covariate with missing data, indicating where a missing value was
##   observed. Running subsequent analyses could then be done using these
##   missing indicators. This allows to assess if the missingness is
##   informative [Groenwold et al (2012)](https://doi.org/10.1503/cmaj.110977).

## We encourage user to run analysis with different imputation methods for a stability check.

## Change variable class to either numeric or factor
adat <- adat %>%
  mutate_if(
    sapply(adat, class) %in% c("integer", "numeric"),
    as.numeric
  ) %>%
  mutate_if(
    sapply(adat, class) %in% c("factor", "character"),
    as.factor
  )


## Missing Value variables check again
missing_value_count <- adat %>%
  summarise(., across(everything(), ~ sum(is.na(.)))) %>% # Use this under R >= 4.0.0
  # summarise_all(funs(sum(is.na(.)))) %>%  # Use this line under R < 4
  as.data.frame() %>%
  `rownames<-`("na_count") %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  arrange(desc(na_count))

head(missing_value_count)

vars_impute <- c(missing_value_count %>%
                   filter(na_count > 0 & rowname != "Y") %>%
                   select(rowname)) # select covariates includes missing value


## optional step: Add indicator variable for missingness
# adat <- adat %>%
#  mutate_at(vars_impute$rowname, list(missing = ~ is.na(.) * 1.))

## Median/Mode imputation based on same variable
df_univariate_impute <- adat

# Median imputation for numeric variables
for (rn in vars_impute$rowname) {
  if (class(df_univariate_impute[[rn]]) == "numeric") {
    df_univariate_impute[rn][is.na(df_univariate_impute[rn])] <- median(df_univariate_impute[[rn]], na.rm = TRUE)
  }
}

# Mode imputation for factor variables
df_univariate_impute <- randomForest::na.roughfix(df_univariate_impute)

## Single imputation based on regression multiple imputation

# Here multiple imputation of the baseline covariates based on all
# other baseline covariates is performed using the mice package. The
# median (continuous variable) or mode (categorical variable) of the 10
# multiply imputed values are finally used for the single imputed data-set.

## remove outcome and trt (as recommended)
exclude_vars <- c("Y", "trt")
df_regress_impute <- adat
## you can add remove.collinear=FALSE if you have collinear variables in dataset to
## prevent imputed dataset contains NA
imputation <- mice::mice(df_regress_impute %>% dplyr::select(-all_of(exclude_vars)),
                         m = 10, maxit = 5, method = "pmm", printFlag = TRUE, seed = 2020
)
# Find mean/mode of 10 imputations
for (v in vars_impute$rowname) {
  if (class(df_regress_impute[[v]]) == "numeric") {
    imp_median <- apply(imputation$imp[[v]], 1, median)
    df_regress_impute[[v]][as.numeric(names(imp_median))] <- imp_median
  }
  if (class(df_regress_impute[[v]]) == "factor") {
    imp_most <- apply(
      imputation$imp[[v]], 1,
      function(x) {
        names(sort(table(x), decreasing = TRUE)[1])
      }
    )
    df_regress_impute[[v]][as.numeric(names(imp_most))] <- imp_most
  }
}


## Save imputed data
saveRDS(df_univariate_impute, file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/analysis_univ_imputed.rds")
saveRDS(df_regress_impute, file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/analysis_regress_imputed.rds")

```

```{r pressure, echo=FALSE}
plot(pressure)
```

## Step 4

```{r explore teh, echo=FALSE}
adat <- readRDS(file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/analysis_regress_imputed.rds")
write.csv(adat, file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/your_dataset.csv", row.names = FALSE)

y <- adat$Y
trt <- select(adat, trt)$trt
trt <- as.numeric(trt == "1")

## First models are build to model the outcome (and for observational
## studies, or when pooling across multiple studies for the treatment
## assignement), based on the stacking procedure as implemented in the
## ['SuperLearner'](https://CRAN.R-project.org/package=SuperLearner) R
## package. Different base models can be utilized for stacking. These
## models are then used to provide pseudo-observations $\phi$ for the
## treatment effect for each patient as in the double-robust learning
## algorithm [Kennedy (2022)](https://arxiv.org/abs/2004.14497). To be
## able to perform statistical inference later the $\phi$ values for
## each patient are created using cross-fitting based on 10-fold
## cross-validation. The prediction for the pseudo-observation for
## every patient is only based on the models where this patient was
## not used for fitting the models (out-of-fold predictions). Note
## that for small data-sets (and or low number of covariates) the
## utilized base models for the super-learner can/should be changed
## (e.g. standard unpenalized regression can also be used).

##############################################################################
## first create pseudo-observations phi (see https://arxiv.org/abs/2004.14497)
##############################################################################

## select covariates to use for modelling the outcome
## make sure to remove outcome
covs_out <- setdiff(colnames(adat), c("Y"))
X_out <- select(adat, all_of(covs_out))
## select covariates to use for modelling treatment (make sure to remove outcome and treatment)
covs_pi <- setdiff(colnames(adat), c("Y", "trt"))
X_pi <- select(adat, all_of(covs_pi))

## super-learners to include
SL.library <- list(
  "SL.glmnet", "SL.xgboost", "SL.ranger"
)


k <- 10
## create k splits, stratified by treatment group
ids <- caret::createFolds(y = adat$trt, k = k)
N <- nrow(adat)
fit_one_fold <- function(i) {
  library(nnls)
  library(SuperLearner)
  train_ids <- setdiff(1:N, ids[[i]])
  test_ids <- ids[[i]]
  ## SuperLearner for outcome
  SL_out <- SuperLearner::SuperLearner(
    Y = y[train_ids],
    X = X_out[train_ids, ],
    SL.library = SL.library, family = "gaussian",
    verbose = FALSE, method = c("method.CC_LS")
  )
  ## SuperLearner for treatment
  SL_pi <- SuperLearner::SuperLearner(
    Y = trt[train_ids],
    X = X_pi[train_ids, ],
    SL.library = SL.library, family = "binomial",
    verbose = FALSE, method = c("method.CC_LS")
  )
  
  ## predictions needed for outcome
  pred_X1 <- pred_X0 <- pred_trt <- X_out[test_ids, ]
  pred_X1$trt <- factor("1", levels = levels(adat$trt))
  pred_X0$trt <- factor("0", levels = levels(adat$trt))
  ## prediction under control
  m0 <- as.numeric(predict(SL_out, newdata = pred_X0)$pred)
  ## prediction under treatment
  m1 <- as.numeric(predict(SL_out, newdata = pred_X1)$pred)
  ## prediction under observed arm
  mtrt <- as.numeric(predict(SL_out, newdata = pred_trt)$pred)
  ## predictions needed for treatment
  pi <- as.numeric(predict(SL_pi, newdata = X_pi[test_ids, ])$pred)
  list(
    i = i, train_ids = train_ids, test_ids = test_ids,
    m0 = m0, m1 = m1, mtrt = mtrt, pi = pi
  )
}
## perform cross-fitting on grid
export <- list(
  adat = adat, X_out = X_out, X_pi = X_pi,
  ids = ids, N = N, SL.library = SL.library, trt = trt, y = y
)
res <- clustermq::Q(fit_one_fold, i = 1:k, n_jobs = k, export = export)

## Calculate pseudo-observations phi
m1 <- m0 <- mtrt <- pi <- numeric(N)
for (i in 1:k) {
  ids <- res[[i]]$test_ids
  m0[ids] <- res[[i]]$m0
  m1[ids] <- res[[i]]$m1
  mtrt[ids] <- res[[i]]$mtrt
  pi[ids] <- res[[i]]$pi
}

## double robust pseudo observations for the treatment difference (see https://arxiv.org/abs/2004.14497)
phi <- m1 - m0 + (trt - pi) / (pi * (1 - pi)) * (y - mtrt)
saveRDS(phi, file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/phi.rds")

## Based on the obtained $\phi$ values, a global heterogeneity test
## is performed and variable importance is assessed. This is done by
## fitting a random forest based on conditional inference trees (as
## implemented in the ['party'](https://CRAN.R-project.org/package=party)
## R package). One advantage of this approach versus the approach
## implemented in the randomForest package is that conditional inference
## trees do not suffer from variable selection bias (towards variables
## with many split possibilities) when choosing the variables to split
## [Hothorn et al (2006)](https://doi.org/10.1198/106186006X133933).
```

```{r explore teh-part2, echo=FALSE}
## Based on the fitted forest then the standard deviation of the
## model-based predictions of the treatment effect for all patients
## are extracted (a large standard deviation would indicate heterogeneity).
## Then the $\phi$ values are permuted against the considered covariates
## and for each permutation the model above is re-fitted and the standard
## deviation of the model-based predictions of the treatment effect is
## calculated. Under the permutation distribution a low standard
## deviation is expected, as the covariates will not explain the $\phi$
## values. A p-value can be extracted based on the proportion of
## permutations standard deviations that are larger than the observed
## standard deviation for the unpermuted data.

## Variable importance is calculated based on the fitted conditional
## random forest using the approaches outlined in [Debeer and Strobl
## (2020)](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-03622-2),
## as implemented in the
## ['party'](https://CRAN.R-project.org/package=party) and
## ['permimp'](https://CRAN.R-project.org/package=permimp) R
## packages. The basic idea of standard variable importance is to permute
## a covariate and then to observe how much the out-of-bag mean squared
## error for the treatment effect increases (averaged over the trees in
## the forest). To better quantify the unique contribution of a variable
## also conditional permutation variable importance is available, where
## a variable is permuted within strata of variables that are correlated
## with the variable of interest.

########################################################################
## perform global heterogeneity test and obtain variable importance
########################################################################
## Fit the conditional forest model to pseudo observations
n_cov <- ncol(X_pi)
## control parameters for conditional forest
## can assess sensitivity of results to mtry (mtry=5 is the default in cforest,
## n_cov/3 is the default in the randomForest package)
## larger ntree will reduce variability in results (e.g. VI rankings)
control_cforest <- party::cforest_unbiased(mtry = 5, ntree = 500)
#fit <- party::cforest(y ~ ., data.frame(X_pi, y = phi), control = control_cforest)
fit <- party::cforest(y ~ ., data.frame(X_pi, y = y), control = control_cforest)
## standard deviation of observed "individual" treatment effects
sd_obs <- sd(predict(fit, OOB = TRUE))
## assess variable importance (to get more stable VI rankings increase nperm)
cf_vi <- permimp::permimp(fit, conditional = FALSE, nperm = 50)
## Optional: The conditional  permutation importance is prohibitive in terms of computational cost.
# cf_vi_cond <- permimp::permimp(fit, conditional = TRUE, nperm = 10)


## assess variability across trees
plot(cf_vi)
plot(cf_vi, type = "box")


## Global heterogeneity test using coin
test <- coin::independence_test(y ~ ., data.frame(X_pi, y = phi), teststat="quadratic")
p_value <- coin::pvalue(test)
p_value

saveRDS(p_value,
        file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/p_value.rds"
)

## Optional: can produce p-values for the importance ranking
## p-values for variable importance
## export <- list(X_pi=X_pi, phi=phi, control_cforest = control_cforest, conditional_permutation = TRUE)
## fit_one_perm_cforest <- function(i){
##   library(permimp, lib.loc="/home/bornkbj3/pub_rlib/")
##   phi_perm <- sample(phi)
##   fitperm <- party::cforest(y ~ ., data.frame(X_pi, y=phi_perm), control = control_cforest)
##   cf_vi_perm <- permimp::permimp(fitperm, conditional = conditional_permutation, n_perm=1)
##   cf_vi_perm$values
## }
## res <- clustermq::Q(fit_one_perm_cforest, i=1:n_perm, n_jobs=min(200,n_perm), export=export)
## cf_vi_perm <- do.call("cbind", res)
## p_values <-  numeric(n_cov)
## for(i in 1:n_cov){
##   sm <- sum(cf_vi_perm[i,] > cf_vi$values[i])
##   ## for conditional permutation use cf_vi_cond$values in line above
##   p_values[i] <- (sm+1)/(n_perm+1)
## }
## ## present p-value on "surprise" scale
## data.frame(variable = names(cf_vi$values), surprise = -log2(p_values))

## Optional: can assess univariate association by LR test based on linear model
## pval <- numeric(n_cov)
## lm_fit_null <- lm(y ~ ., data = data.frame(y=phi))
## LR_null <- logLik(lm_fit_null)
## for (j in 1:n_cov) {
##   lm_fit <- lm(y ~ ., data = data.frame(x=X_pi[,j], y=phi))
##   pval[j] <- anova(lm_fit, lm_fit_null)[["Pr(>F)"]][2]
## }
## ## present p-value on "surprise" scale
## data.frame(variable = colnames(X_pi), surprise = -log2(pval))

importance_scores <- data.frame(
  variable = names(cf_vi$values),
  vi = cf_vi$values #, cond_vi = cf_vi_cond$values
)
saveRDS(importance_scores, file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/importance.rds")



# 
# ### Optional: interaction variable importance based on partial dependence function
# ### refer paper: https://arxiv.org/pdf/1805.04755.pdf
# ## only based on top nx variables from importance score
#nx <- 10
#importance_scores <- as_tibble(importance_scores)
#topn <- importance_scores %>% 
#  dplyr::arrange(desc(vi)) %>% 
#  dplyr::slice(1:10) %>% 
#  dplyr::pull(variable)
#topn <- importance_scores[1:10,2] %>% arrange(desc(vi)) %>% slice(1:nx) %>% pull(variable)
#comb_var <- combn(topn, 2)

#fit_one_inter <- function(i) {
#  library(moreparty)
#  library(pdp)
#  library(vivid)
#  GetInteractionStrength(fit, xnames = comb_var[, i])
#}

#res <- clustermq::Q(fit_one_inter, i = 1:ncol(comb_var), n_jobs = min(200, ncol(comb_var)),
#                    export = list(fit = fit, comb_var = comb_var))

#saveRDS(res %>% bind_rows(), file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/importance_inter.rds")
```
## Step 5

```{r pressure, echo=FALSE}
## Summary plots see docs/FURTHER_INFO.md for more information.
## load data without missing covariates imputed
adat <- readRDS(file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/analysis_data2.rds")
importance_scores <- readRDS(file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/importance.rds")
phi <- readRDS(file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/phi.rds")
p_value <- readRDS(file = "/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/p_value.rds")

# create a template, users can change it according to their preferences
plot_theme <- theme(
  # Hide panel borders and remove grid lines
  axis.text=element_text(size=12),
  axis.title=element_text(size=14),
  legend.title = element_text(size = 14),
  legend.text = element_text( size = 14),
  strip.text.x = element_text( size = 14),
  plot.title = element_text(size=14))

#####################
## Evidence of TEH ##
#####################
# No plot for visualizing the vidence, just the p-value

######################
## Effect modifiers ##
######################
## variable importance plot
## standard permutation variable importance
p_importance <- ggplot(head(importance_scores %>% arrange(-vi) %>% filter(vi>0), 10),
                       aes(reorder(variable, vi), vi)) +
  geom_col(color="black", fill = "lightblue" ) + 
  xlab("Variables") + ylab("") +
  # ylab("Variable importance\n(average increase in OOB-MSE\n under permutation)")+
  theme_classic() + coord_flip() + plot_theme 

ggsave("/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/var_importance_region_response.png", p_importance, width = 6, height = 4, units = "in")
# saveRDS(p_importance, file = "reports/figures/var_importance.rds")

## select top 10 variables of interest
vars_to_plot <- rownames(head(importance_scores %>% arrange(-vi), 10))

## scatterplot matrix of variables of interest to identify potential
## correlations/dependencies
p_correlation <- ggpairs(select(adat, vars_to_plot))
ggsave("/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/correlation_graph.pdf", p_correlation, width = 14, height = 14, units = "in")
```

```{r pressure, echo=FALSE}
################################
## Exploratory display of TEH ##
################################
## univariate plot
## - average outcome by treatment group for variables of interest
## - treatment difference for variables of interest
## for paper, choose one continuous and one categorical
vars_to_plot <- c("X1", "X27")
p_uni <- vector(mode = "list", length = 4)
for (i in 1:2) {
  p_uni[[i*2-1]] <- p_yx(
    y.name = "Y", x.name = vars_to_plot[i], trt.name = "trt",
    data = adat, df = 3, family = "gaussian"
  ) + plot_theme
  p_uni[[i*2]] <- p_diffx(
    y.name = "Y", x.name = vars_to_plot[i], trt.name = "trt",
    data = adat, df = 3, family = "gaussian", phi_predictions = NULL
  ) + plot_theme
}

for(i in 1:4){
  ggsave(sprintf("/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/display_uni_%d.pdf", i), p_uni[[i]], width = 6, height = 4, units = "in")
}

########################################################################
## assess interactions using estimated phi's
## note that estimated phis can be outside [-1,1], so that also the
## smoothed phis may be outside [-1,1]

## plot bivariate plots for variables of interest
pairs_to_plot <- list(c("X1", "X9"), c("X1", "X4"))

## Optional: variable importance & interaction effect plots
## based on the interaction effect calculated from 04_explore_TEH.R
imp_inter <- readRDS("/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/importance_inter.rds")
imp_inter_dat <- imp_inter %>%
  tidyr::separate(Variables, sep = "\\*", into = c("var1", "var2")) %>%
  rename(imp = Interaction)
imp_inter_dat2 <- lapply(1:nrow(imp_inter_dat), function(ii){
  xx <- imp_inter_dat[ii, ]
  row2 <- xx
  names(row2) <- c("var2", "var1", "imp")
  rbind(xx, row2)
}) %>% bind_rows()

vars_sel <- unique(c(imp_inter_dat$var1, imp_inter_dat$var2))
imp_dat <- importance_scores %>% filter(variable %in% vars_sel) %>%
  select(var1 = variable, imp = vi) %>% mutate(var2 = var1) %>% rbind(imp_inter_dat2) %>%
  tidyr::pivot_wider(names_from = var2, values_from = imp) %>%
  tibble::column_to_rownames(var = "var1") %>%
  as.matrix()

order_desc <- order(diag(imp_dat), decreasing = T)
imp_order <- imp_dat[order_desc, order_desc]

pdf("/Users/tianyuzheng/Documents/PKU RA/Novartis/Figures/var_importance_inter.pdf")
viviHeatmap(mat = imp_order, angle = 45)
dev.off()

p1 <- ggplot(adat, aes(x =X32, fill = X31)) +
  geom_density(alpha = 0.5) +
  labs(title = "Overlap of X32 by Region", x = "X31", y = "Density") +
  theme_minimal()
p1

```